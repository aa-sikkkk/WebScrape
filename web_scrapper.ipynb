{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scrapper with Command Line Interface\n"
      ],
      "metadata": {
        "id": "CAvQuD_rWxpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 beautifultable certifi cffi charset-normalizer cryptography h11 idna outcome pycparser pyOpenSSL PySocks requests selenium sniffio sortedcontainers soupsieve trio trio-websocket urllib3 wcwidth wsproto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywgOYf30XCFB",
        "outputId": "1ad405c1-0bb3-456b-929d-7c9854f5d886"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting beautifultable\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (2024.8.30)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (43.0.1)\n",
            "Collecting h11\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (3.8)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (2.22)\n",
            "Requirement already satisfied: pyOpenSSL in /usr/local/lib/python3.10/dist-packages (24.2.1)\n",
            "Requirement already satisfied: PySocks in /usr/local/lib/python3.10/dist-packages (1.7.1)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.24.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: soupsieve in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Collecting trio\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (0.2.13)\n",
            "Collecting wsproto\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from outcome) (24.2.0)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio) (1.2.2)\n",
            "Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading selenium-4.24.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: outcome, h11, beautifultable, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed beautifultable-1.1.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.24.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required modules\n",
        "import json\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from beautifultable import BeautifulTable\n",
        "\n",
        "\n",
        "\n",
        "def load_json(database_json_file=\"scraped_data.json\"):\n",
        "    \"\"\"\n",
        "    This function will load json data from scraped_data.json\n",
        "    if file exist it will load data from it else creates an empty one\"\"\"\n",
        "    try:\n",
        "        with open(database_json_file, \"r\") as read_it:\n",
        "            all_data_base = json.loads(read_it.read())\n",
        "            return all_data_base\n",
        "    except:\n",
        "        all_data_base = dict()\n",
        "        return all_data_base\n",
        "\n",
        "\n",
        "def save_scraped_data_in_json(data, database_json_file=\"scraped_data.json\"):\n",
        "    \"\"\"\n",
        "        This function Save the scraped data in json format. scraped_data.json file if it exist else create it.\n",
        "        if file already exist you can view previous scraped data in it.\n",
        "    \"\"\"\n",
        "    file_obj =  open(database_json_file, \"w\")\n",
        "    file_obj.write(json.dumps(data))\n",
        "    file_obj.close()\n",
        "\n",
        "\n",
        "def existing_scraped_data_init(json_db):\n",
        "    \"\"\"\n",
        "    This function will check if scraped_data key exist in json_db or not\n",
        "    \"\"\"\n",
        "    scraped_data = json_db.get(\"scraped_data\")\n",
        "    if scraped_data is None:\n",
        "        json_db['scraped_data'] = dict()\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def scraped_time_is():\n",
        "    \"\"\"\n",
        "    This function will return current date and time in string format\n",
        "    \"\"\"\n",
        "    now = datetime.now()\n",
        "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    return dt_string\n",
        "\n",
        "def process_url_request(website_url):\n",
        "    \"\"\"\n",
        "    This function will process the url request and return the soup object\n",
        "    \"\"\"\n",
        "    requets_data = requests.get(website_url)\n",
        "    if requets_data.status_code == 200:\n",
        "        soup = BeautifulSoup(requets_data.text, 'lxml')\n",
        "        return soup\n",
        "    return None\n",
        "\n",
        "def proccess_beautiful_soup_data(soup):\n",
        "    return {\n",
        "        'title': soup.find('title').text if soup.find('title') else 'No title found',\n",
        "        'all_anchor_href': [i['href'] for i in soup.find_all('a', href=True)],\n",
        "        'all_anchors': [str(i) for i in soup.find_all('a')],\n",
        "        'all_images_data': [str(i) for i in soup.find_all('img')],\n",
        "        'all_images_source_data': [i.get('src') for i in soup.find_all('img')],\n",
        "        'all_h1_data': [i.text for i in soup.find_all('h1')],\n",
        "        'all_h2_data': [i.text for i in soup.find_all('h2')],\n",
        "        'all_h3_data': [i.text for i in soup.find_all('h3')],\n",
        "        'all_p_data': [i.text for i in soup.find_all('p')]\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Main program starts from here\n",
        "while True:\n",
        "\n",
        "    print(\"\"\"  ================ Welcome to this scraping program =============\n",
        "    ==>> press 1 for checking existing scraped websites\n",
        "    ==>> press 2 for scrap a single website\n",
        "    ==>> press 3 for exit\n",
        "    \"\"\")\n",
        "\n",
        "    choice = int(input(\"==>> Please enter your choice :\"))\n",
        "\n",
        "    # load data\n",
        "    local_json_db = load_json()\n",
        "    existing_scraped_data_init(local_json_db)\n",
        "\n",
        "    if choice == 1:\n",
        "        # Using Beautiful table for presenting scraped data in a good way !!\n",
        "        # Read more about from this link https://beautifultable.readthedocs.io/en/latest/index.html\n",
        "        scraped_websites_table = BeautifulTable()\n",
        "        scraped_websites_table.columns.header = [\"Sr no.\", \"Allias name \", \"Website domain\", \"title\",   \"Scraped at\", \"Status\"]\n",
        "        scraped_websites_table.set_style(BeautifulTable.STYLE_BOX_DOUBLED)\n",
        "\n",
        "\n",
        "        local_json_db = load_json()\n",
        "        for count,  data in enumerate(local_json_db['scraped_data']):\n",
        "           scraped_websites_table.rows.append([count + 1,\n",
        "                            local_json_db['scraped_data'][data]['alias'],\n",
        "                            local_json_db['scraped_data'][data]['domain'],\n",
        "                            local_json_db['scraped_data'][data]['title'],\n",
        "                            local_json_db['scraped_data'][data]['scraped_at'],\n",
        "                            local_json_db['scraped_data'][data]['status']])\n",
        "        # all_scraped_websites = [websites['name'] for websites in local_json_db['scraped_data']]\n",
        "        if not local_json_db['scraped_data']:\n",
        "            print('===> No existing data found !!!')\n",
        "        print(scraped_websites_table)\n",
        "\n",
        "    elif choice == 2:\n",
        "        print()\n",
        "        url_for_scrap = input(\"===> Please enter url you want to scrap:\")\n",
        "        is_accessable = process_url_request(url_for_scrap)\n",
        "        if is_accessable:\n",
        "            scraped_data_packet = proccess_beautiful_soup_data(is_accessable)\n",
        "            print()\n",
        "            print(' =====> Data scraped successfully !!!')\n",
        "            key_for_storing_data = input(\"enter alias name for saving scraped data :\")\n",
        "            scraped_data_packet['url'] = url_for_scrap\n",
        "            scraped_data_packet['name'] = key_for_storing_data\n",
        "            scraped_data_packet['scraped_at'] = scraped_time_is()\n",
        "            if key_for_storing_data in  local_json_db['scraped_data']:\n",
        "                key_for_storing_data = key_for_storing_data + str(scraped_time_is())\n",
        "                print(\"Provided key is already exist so data stored as : {}\".format(key_for_storing_data))\n",
        "            scraped_data_packet['alias'] = key_for_storing_data\n",
        "            scraped_data_packet['status'] = True\n",
        "            scraped_data_packet['domain'] = urlparse(url_for_scrap).netloc\n",
        "\n",
        "            local_json_db['scraped_data'][key_for_storing_data] = scraped_data_packet\n",
        "            print(\n",
        "                'scraped data is:', local_json_db['scraped_data'][key_for_storing_data]\n",
        "            )\n",
        "            save_scraped_data_in_json(local_json_db)\n",
        "            # load data\n",
        "            local_json_db = load_json()\n",
        "            print(' =====> Data saved successfully !!!')\n",
        "            print()\n",
        "    elif choice == 3:\n",
        "        print('Thank you for using !!!')\n",
        "        break\n",
        "\n",
        "    elif choice == 4:\n",
        "        print('Thank you for using !!!')\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        print(\"enter a valid choice \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vWnTR2WYR_3",
        "outputId": "05361bb3-7e73-4cce-df1b-ec8605e237ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ================ Welcome to this scraping program =============\n",
            "    ==>> press 1 for checking existing scraped websites\n",
            "    ==>> press 2 for scrap a single website\n",
            "    ==>> press 3 for exit\n",
            "    \n",
            "==>> Please enter your choice :2\n",
            "\n",
            "===> Please enter url you want to scrap:https://google.com/\n",
            "\n",
            " =====> Data scraped successfully !!!\n",
            "enter alias name for saving scraped data :googl.\n",
            "scraped data is: {'title': 'Google', 'all_anchor_href': ['https://www.google.com/imghp?hl=en&tab=wi', 'https://maps.google.com/maps?hl=en&tab=wl', 'https://play.google.com/?hl=en&tab=w8', 'https://www.youtube.com/?tab=w1', 'https://news.google.com/?tab=wn', 'https://mail.google.com/mail/?tab=wm', 'https://drive.google.com/?tab=wo', 'https://www.google.com/intl/en/about/products?tab=wh', 'http://www.google.com/history/optout?hl=en', '/preferences?hl=en', 'https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=https://www.google.com/&ec=GAZAAQ', '/advanced_search?hl=en&authuser=0', 'https://www.google.com/url?q=https://www.google.com/search%3Fq%3Dhow%2Bto%2Bregister%2Bto%2Bvote%2Bin%2Bthe%2Bus%26hl%3Den%26gl%3Dus%26stick%3DH4sIAAAAAAAAAHvE6MYt8PLHPWEp20lrTl5jNOcSc81JTS7JzM8LSk3PLC5JLQrJD8svSRWS5eKAyQgJSvFz8eqn6xsa5pYV5GZlVOXwLGJVzMgvVyjJVyiC6gOxy4A6FTLzFEoyUhVKiwEnZn1YbQAAAA%26utm_source%3Dsearch%26utm_medium%3Dhpp%26utm_campaign%3D18&source=hpp&id=19043815&ct=3&usg=AOvVaw2EF95qOkThnCEyydlJGjug&sa=X&ved=0ahUKEwixyJ7p2cmIAxWkmbAFHfEhAeUQ8IcBCAY', '/intl/en/ads/', '/services/', '/intl/en/about.html', '/intl/en/policies/privacy/', '/intl/en/policies/terms/'], 'all_anchors': ['<a class=\"gb1\" href=\"https://www.google.com/imghp?hl=en&amp;tab=wi\">Images</a>', '<a class=\"gb1\" href=\"https://maps.google.com/maps?hl=en&amp;tab=wl\">Maps</a>', '<a class=\"gb1\" href=\"https://play.google.com/?hl=en&amp;tab=w8\">Play</a>', '<a class=\"gb1\" href=\"https://www.youtube.com/?tab=w1\">YouTube</a>', '<a class=\"gb1\" href=\"https://news.google.com/?tab=wn\">News</a>', '<a class=\"gb1\" href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a>', '<a class=\"gb1\" href=\"https://drive.google.com/?tab=wo\">Drive</a>', '<a class=\"gb1\" href=\"https://www.google.com/intl/en/about/products?tab=wh\" style=\"text-decoration:none\"><u>More</u> »</a>', '<a class=\"gb4\" href=\"http://www.google.com/history/optout?hl=en\">Web History</a>', '<a class=\"gb4\" href=\"/preferences?hl=en\">Settings</a>', '<a class=\"gb4\" href=\"https://accounts.google.com/ServiceLogin?hl=en&amp;passive=true&amp;continue=https://www.google.com/&amp;ec=GAZAAQ\" id=\"gb_70\" target=\"_top\">Sign in</a>', '<a href=\"/advanced_search?hl=en&amp;authuser=0\">Advanced search</a>', '<a href=\"https://www.google.com/url?q=https://www.google.com/search%3Fq%3Dhow%2Bto%2Bregister%2Bto%2Bvote%2Bin%2Bthe%2Bus%26hl%3Den%26gl%3Dus%26stick%3DH4sIAAAAAAAAAHvE6MYt8PLHPWEp20lrTl5jNOcSc81JTS7JzM8LSk3PLC5JLQrJD8svSRWS5eKAyQgJSvFz8eqn6xsa5pYV5GZlVOXwLGJVzMgvVyjJVyiC6gOxy4A6FTLzFEoyUhVKiwEnZn1YbQAAAA%26utm_source%3Dsearch%26utm_medium%3Dhpp%26utm_campaign%3D18&amp;source=hpp&amp;id=19043815&amp;ct=3&amp;usg=AOvVaw2EF95qOkThnCEyydlJGjug&amp;sa=X&amp;ved=0ahUKEwixyJ7p2cmIAxWkmbAFHfEhAeUQ8IcBCAY\" rel=\"nofollow\">Register to vote</a>', '<a href=\"/intl/en/ads/\">Advertising</a>', '<a href=\"/services/\">Business Solutions</a>', '<a href=\"/intl/en/about.html\">About Google</a>', '<a href=\"/intl/en/policies/privacy/\">Privacy</a>', '<a href=\"/intl/en/policies/terms/\">Terms</a>'], 'all_images_data': ['<img alt=\"Google\" height=\"92\" id=\"hplogo\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\"/>'], 'all_images_source_data': ['/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png'], 'all_h1_data': [], 'all_h2_data': [], 'all_h3_data': [], 'all_p_data': ['© 2024 - Privacy - Terms'], 'url': 'https://google.com/', 'name': 'googl.', 'scraped_at': '17/09/2024 09:50:09', 'alias': 'googl.', 'status': True, 'domain': 'google.com'}\n",
            " =====> Data saved successfully !!!\n",
            "\n",
            "  ================ Welcome to this scraping program =============\n",
            "    ==>> press 1 for checking existing scraped websites\n",
            "    ==>> press 2 for scrap a single website\n",
            "    ==>> press 3 for exit\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_BjHrCwXNHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}