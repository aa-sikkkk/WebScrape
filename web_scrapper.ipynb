{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAvQuD_rWxpF"
      },
      "source": [
        "### Web Scrapper with Command Line Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywgOYf30XCFB",
        "outputId": "1ad405c1-0bb3-456b-929d-7c9854f5d886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting beautifultable\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (2024.8.30)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (43.0.1)\n",
            "Collecting h11\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (3.8)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (2.22)\n",
            "Requirement already satisfied: pyOpenSSL in /usr/local/lib/python3.10/dist-packages (24.2.1)\n",
            "Requirement already satisfied: PySocks in /usr/local/lib/python3.10/dist-packages (1.7.1)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.24.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: soupsieve in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Collecting trio\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (0.2.13)\n",
            "Collecting wsproto\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from outcome) (24.2.0)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio) (1.2.2)\n",
            "Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading selenium-4.24.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: outcome, h11, beautifultable, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed beautifultable-1.1.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.24.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 beautifultable certifi cffi charset-normalizer cryptography h11 idna outcome pycparser pyOpenSSL PySocks requests selenium sniffio sortedcontainers soupsieve trio trio-websocket urllib3 wcwidth wsproto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vWnTR2WYR_3",
        "outputId": "05361bb3-7e73-4cce-df1b-ec8605e237ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ================ Welcome to this scraping program =============\n",
            "    ==>> press 1 for checking existing scraped websites\n",
            "    ==>> press 2 for scrap a single website\n",
            "    ==>> press 3 for exit\n",
            "    \n",
            "==>> Please enter your choice :2\n",
            "\n",
            "===> Please enter url you want to scrap:https://google.com/\n",
            "\n",
            " =====> Data scraped successfully !!!\n",
            "enter alias name for saving scraped data :googl.\n",
            "scraped data is: {'title': 'Google', 'all_anchor_href': ['https://www.google.com/imghp?hl=en&tab=wi', 'https://maps.google.com/maps?hl=en&tab=wl', 'https://play.google.com/?hl=en&tab=w8', 'https://www.youtube.com/?tab=w1', 'https://news.google.com/?tab=wn', 'https://mail.google.com/mail/?tab=wm', 'https://drive.google.com/?tab=wo', 'https://www.google.com/intl/en/about/products?tab=wh', 'http://www.google.com/history/optout?hl=en', '/preferences?hl=en', 'https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=https://www.google.com/&ec=GAZAAQ', '/advanced_search?hl=en&authuser=0', 'https://www.google.com/url?q=https://www.google.com/search%3Fq%3Dhow%2Bto%2Bregister%2Bto%2Bvote%2Bin%2Bthe%2Bus%26hl%3Den%26gl%3Dus%26stick%3DH4sIAAAAAAAAAHvE6MYt8PLHPWEp20lrTl5jNOcSc81JTS7JzM8LSk3PLC5JLQrJD8svSRWS5eKAyQgJSvFz8eqn6xsa5pYV5GZlVOXwLGJVzMgvVyjJVyiC6gOxy4A6FTLzFEoyUhVKiwEnZn1YbQAAAA%26utm_source%3Dsearch%26utm_medium%3Dhpp%26utm_campaign%3D18&source=hpp&id=19043815&ct=3&usg=AOvVaw2EF95qOkThnCEyydlJGjug&sa=X&ved=0ahUKEwixyJ7p2cmIAxWkmbAFHfEhAeUQ8IcBCAY', '/intl/en/ads/', '/services/', '/intl/en/about.html', '/intl/en/policies/privacy/', '/intl/en/policies/terms/'], 'all_anchors': ['<a class=\"gb1\" href=\"https://www.google.com/imghp?hl=en&amp;tab=wi\">Images</a>', '<a class=\"gb1\" href=\"https://maps.google.com/maps?hl=en&amp;tab=wl\">Maps</a>', '<a class=\"gb1\" href=\"https://play.google.com/?hl=en&amp;tab=w8\">Play</a>', '<a class=\"gb1\" href=\"https://www.youtube.com/?tab=w1\">YouTube</a>', '<a class=\"gb1\" href=\"https://news.google.com/?tab=wn\">News</a>', '<a class=\"gb1\" href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a>', '<a class=\"gb1\" href=\"https://drive.google.com/?tab=wo\">Drive</a>', '<a class=\"gb1\" href=\"https://www.google.com/intl/en/about/products?tab=wh\" style=\"text-decoration:none\"><u>More</u> »</a>', '<a class=\"gb4\" href=\"http://www.google.com/history/optout?hl=en\">Web History</a>', '<a class=\"gb4\" href=\"/preferences?hl=en\">Settings</a>', '<a class=\"gb4\" href=\"https://accounts.google.com/ServiceLogin?hl=en&amp;passive=true&amp;continue=https://www.google.com/&amp;ec=GAZAAQ\" id=\"gb_70\" target=\"_top\">Sign in</a>', '<a href=\"/advanced_search?hl=en&amp;authuser=0\">Advanced search</a>', '<a href=\"https://www.google.com/url?q=https://www.google.com/search%3Fq%3Dhow%2Bto%2Bregister%2Bto%2Bvote%2Bin%2Bthe%2Bus%26hl%3Den%26gl%3Dus%26stick%3DH4sIAAAAAAAAAHvE6MYt8PLHPWEp20lrTl5jNOcSc81JTS7JzM8LSk3PLC5JLQrJD8svSRWS5eKAyQgJSvFz8eqn6xsa5pYV5GZlVOXwLGJVzMgvVyjJVyiC6gOxy4A6FTLzFEoyUhVKiwEnZn1YbQAAAA%26utm_source%3Dsearch%26utm_medium%3Dhpp%26utm_campaign%3D18&amp;source=hpp&amp;id=19043815&amp;ct=3&amp;usg=AOvVaw2EF95qOkThnCEyydlJGjug&amp;sa=X&amp;ved=0ahUKEwixyJ7p2cmIAxWkmbAFHfEhAeUQ8IcBCAY\" rel=\"nofollow\">Register to vote</a>', '<a href=\"/intl/en/ads/\">Advertising</a>', '<a href=\"/services/\">Business Solutions</a>', '<a href=\"/intl/en/about.html\">About Google</a>', '<a href=\"/intl/en/policies/privacy/\">Privacy</a>', '<a href=\"/intl/en/policies/terms/\">Terms</a>'], 'all_images_data': ['<img alt=\"Google\" height=\"92\" id=\"hplogo\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\"/>'], 'all_images_source_data': ['/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png'], 'all_h1_data': [], 'all_h2_data': [], 'all_h3_data': [], 'all_p_data': ['© 2024 - Privacy - Terms'], 'url': 'https://google.com/', 'name': 'googl.', 'scraped_at': '17/09/2024 09:50:09', 'alias': 'googl.', 'status': True, 'domain': 'google.com'}\n",
            " =====> Data saved successfully !!!\n",
            "\n",
            "  ================ Welcome to this scraping program =============\n",
            "    ==>> press 1 for checking existing scraped websites\n",
            "    ==>> press 2 for scrap a single website\n",
            "    ==>> press 3 for exit\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Import required modules\n",
        "import json\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from beautifultable import BeautifulTable\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the LLaMA model from Hugging Face\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "access_token = \"Your_huggingface_token_here\" # Create Token from Hugging Face \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token)\n",
        "\n",
        "# LLaMA3 Prompt template\n",
        "template = (\n",
        "    \"You are tasked with extracting specific information from the following text content: {dom_content}. \"\n",
        "    \"Please follow these instructions carefully: \\n\\n\"\n",
        "    \"1. **Extract Information:** Only extract the information that directly matches the provided description: {parse_description}. \"\n",
        "    \"2. **No Extra Content:** Do not include any additional text, comments, or explanations in your response. \"\n",
        "    \"3. **Empty Response:** If no information matches the description, return an empty string ('').\"\n",
        "    \"4. **Direct Data Only:** Your output should contain only the data that is explicitly requested, with no other text.\"\n",
        ")\n",
        "\n",
        "# Load existing scraped data from JSON\n",
        "def load_json(database_json_file=\"scraped_data.json\"):\n",
        "    try:\n",
        "        with open(database_json_file, \"r\") as read_it:\n",
        "            all_data_base = json.loads(read_it.read())\n",
        "            return all_data_base\n",
        "    except FileNotFoundError:\n",
        "        return {\"scraped_data\": {}}\n",
        "\n",
        "# Save scraped data to JSON\n",
        "def save_scraped_data_in_json(data, database_json_file=\"scraped_data.json\"):\n",
        "    with open(database_json_file, \"w\") as file_obj:\n",
        "        file_obj.write(json.dumps(data, indent=4))\n",
        "\n",
        "# Initialize scraped data in JSON\n",
        "def existing_scraped_data_init(json_db):\n",
        "    if json_db.get(\"scraped_data\") is None:\n",
        "        json_db['scraped_data'] = {}\n",
        "\n",
        "# Get current time in a specific format\n",
        "def scraped_time_is():\n",
        "    now = datetime.now()\n",
        "    return now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "\n",
        "# Make a request to the URL and get the page content\n",
        "def process_url_request(website_url):\n",
        "    requets_data = requests.get(website_url)\n",
        "    if requets_data.status_code == 200:\n",
        "        soup = BeautifulSoup(requets_data.text, 'html.parser')\n",
        "        return soup\n",
        "    return None\n",
        "\n",
        "# Process the BeautifulSoup object to extract relevant data\n",
        "def proccess_beautiful_soup_data(soup):\n",
        "    return {\n",
        "        'title': soup.find('title').text if soup.find('title') else 'No title found',\n",
        "        'all_anchor_href': [i['href'] for i in soup.find_all('a', href=True)],\n",
        "        'all_anchors': [str(i) for i in soup.find_all('a')],\n",
        "        'all_images_data': [str(i) for i in soup.find_all('img')],\n",
        "        'all_images_source_data': [i.get('src') for i in soup.find_all('img')],\n",
        "        'all_h1_data': [i.text for i in soup.find_all('h1')],\n",
        "        'all_h2_data': [i.text for i in soup.find_all('h2')],\n",
        "        'all_h3_data': [i.text for i in soup.find_all('h3')],\n",
        "        'all_p_data': [i.text for i in soup.find_all('p')]\n",
        "    }\n",
        "\n",
        "# Function to interact with LLaMA model for parsing\n",
        "def prompt_model_for_parsing(dom_content, parse_description):\n",
        "    # Use the template to generate the prompt\n",
        "    prompt = template.format(dom_content=dom_content, parse_description=parse_description)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "    model_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # This is to check if the output is in valid JSON format\n",
        "    try:\n",
        "        # Attempt to parse the model's output as JSON\n",
        "        parsed_output = json.loads(model_output)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Model output is not valid JSON. Saving as text.\")\n",
        "        parsed_output = {'parsed_data': model_output}  # Save raw text inside a JSON structure\n",
        "\n",
        "    return parsed_output\n",
        "\n",
        "# Main Logic starts here\n",
        "while True:\n",
        "    print(\"\"\"  ================ Welcome to this scraping program =============\n",
        "    ==>> press 1 for checking existing scraped websites\n",
        "    ==>> press 2 to scrape a single website\n",
        "    ==>> press 3 to exit\n",
        "    \"\"\")\n",
        "\n",
        "    choice = int(input(\"==>> Please enter your choice :\"))\n",
        "\n",
        "    # Load data\n",
        "    local_json_db = load_json()\n",
        "    existing_scraped_data_init(local_json_db)\n",
        "\n",
        "    if choice == 1:\n",
        "        scraped_websites_table = BeautifulTable()\n",
        "        scraped_websites_table.columns.header = [\"Sr no.\", \"Website domain\", \"Title\", \"Scraped at\", \"Status\"]\n",
        "        scraped_websites_table.set_style(BeautifulTable.STYLE_BOX_DOUBLED)\n",
        "\n",
        "        for count, data in enumerate(local_json_db['scraped_data']):\n",
        "           scraped_websites_table.rows.append([count + 1,\n",
        "                            local_json_db['scraped_data'][data]['domain'],\n",
        "                            local_json_db['scraped_data'][data]['title'],\n",
        "                            local_json_db['scraped_data'][data]['scraped_at'],\n",
        "                            local_json_db['scraped_data'][data]['status']])\n",
        "        if not local_json_db['scraped_data']:\n",
        "            print('===> No existing data found !!!')\n",
        "        print(scraped_websites_table)\n",
        "\n",
        "    elif choice == 2:\n",
        "        parse_description = input(\"Enter the specific data you want to extract (e.g., all headers, links, etc.): \")\n",
        "        url_for_scrap = input(\"===> Please enter the URL you want to scrape:\")\n",
        "\n",
        "        # Make the request and process the data\n",
        "        is_accessable = process_url_request(url_for_scrap)\n",
        "        if is_accessable:\n",
        "            scraped_data_packet = proccess_beautiful_soup_data(is_accessable)\n",
        "            print(' =====> Data scraped successfully !!!')\n",
        "\n",
        "            scraped_data_packet['url'] = url_for_scrap\n",
        "            scraped_data_packet['scraped_at'] = scraped_time_is()\n",
        "            scraped_data_packet['status'] = True\n",
        "            scraped_data_packet['domain'] = urlparse(url_for_scrap).netloc\n",
        "\n",
        "            # Extract the relevant data based on user instructions\n",
        "            dom_content = str(is_accessable)\n",
        "            model_output = prompt_model_for_parsing(dom_content, parse_description)\n",
        "\n",
        "            # Automatically save as JSON\n",
        "            timestamp = scraped_time_is().replace(\"/\", \"_\").replace(\" \", \"_\").replace(\":\", \"_\")\n",
        "            json_filename = f\"parsed_data_{timestamp}.json\"\n",
        "\n",
        "            # Save the parsed output\n",
        "            with open(json_filename, \"w\") as f:\n",
        "                json.dump(model_output, f, indent=4)\n",
        "            print(f'Parsed data saved to {json_filename}.')\n",
        "\n",
        "    elif choice == 3:\n",
        "        print('Thank you for using the scraper!')\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        print(\"Please enter a valid choice.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_BjHrCwXNHA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
